# PIPELINE DEFINITION
# Name: classification-model-eval-metrics
# Inputs:
#    location: str
#    model: system.Artifact
#    project: str
#    thresholds_dict_str: str
# Outputs:
#    classification-model-eval-metrics-metrics: system.Metrics
#    classification-model-eval-metrics-metricsc: system.ClassificationMetrics
#    dep_decision: str
#    metrics: system.Metrics
#    metricsc: system.ClassificationMetrics
components:
  comp-classification-model-eval-metrics:
    executorLabel: exec-classification-model-eval-metrics
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        location:
          parameterType: STRING
        project:
          parameterType: STRING
        thresholds_dict_str:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        metricsc:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
      parameters:
        dep_decision:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-classification-model-eval-metrics:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - classification_model_eval_metrics
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef classification_model_eval_metrics(\n    project: str,\n    location:\
          \ str,\n    thresholds_dict_str: str,\n    model: Input[Artifact],\n   \
          \ metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n\
          ) -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n\
          \n    import json\n    import logging\n\n    from google.cloud import aiplatform\n\
          \n    aiplatform.init(project=project)\n\n    # Fetch model eval info\n\
          \    def get_eval_info(model):\n        response = model.list_model_evaluations()\n\
          \        metrics_list = []\n        metrics_string_list = []\n        for\
          \ evaluation in response:\n            evaluation = evaluation.to_dict()\n\
          \            print(\"model_evaluation\")\n            print(\" name:\",\
          \ evaluation[\"name\"])\n            print(\" metrics_schema_uri:\", evaluation[\"\
          metricsSchemaUri\"])\n            metrics = evaluation[\"metrics\"]\n  \
          \          for metric in metrics.keys():\n                logging.info(\"\
          metric: %s, value: %s\", metric, metrics[metric])\n            metrics_str\
          \ = json.dumps(metrics)\n            metrics_list.append(metrics)\n    \
          \        metrics_string_list.append(metrics_str)\n\n        return (\n \
          \           evaluation[\"name\"],\n            metrics_list,\n         \
          \   metrics_string_list,\n        )\n\n    # Use the given metrics threshold(s)\
          \ to determine whether the model is\n    # accurate enough to deploy.\n\
          \    def classification_thresholds_check(metrics_dict, thresholds_dict):\n\
          \        for k, v in thresholds_dict.items():\n            logging.info(\"\
          k {}, v {}\".format(k, v))\n            if k in [\"auRoc\", \"auPrc\"]:\
          \  # higher is better\n                if metrics_dict[k] < v:  # if under\
          \ threshold, don't deploy\n                    logging.info(\"{} < {}; returning\
          \ False\".format(metrics_dict[k], v))\n                    return False\n\
          \        logging.info(\"threshold checks passed.\")\n        return True\n\
          \n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix\
          \ = metrics_list[0][\"confusionMatrix\"]\n        logging.info(\"rows: %s\"\
          , test_confusion_matrix[\"rows\"])\n\n        # log the ROC curve\n    \
          \    fpr = []\n        tpr = []\n        thresholds = []\n        for item\
          \ in metrics_list[0][\"confidenceMetrics\"]:\n            fpr.append(item.get(\"\
          falsePositiveRate\", 0.0))\n            tpr.append(item.get(\"recall\",\
          \ 0.0))\n            thresholds.append(item.get(\"confidenceThreshold\"\
          , 0.0))\n        print(f\"fpr: {fpr}\")\n        print(f\"tpr: {tpr}\")\n\
          \        print(f\"thresholds: {thresholds}\")\n        metricsc.log_roc_curve(fpr,\
          \ tpr, thresholds)\n\n        # log the confusion matrix\n        annotations\
          \ = []\n        for item in test_confusion_matrix[\"annotationSpecs\"]:\n\
          \            annotations.append(item[\"displayName\"])\n        logging.info(\"\
          confusion matrix annotations: %s\", annotations)\n        metricsc.log_confusion_matrix(\n\
          \            annotations,\n            test_confusion_matrix[\"rows\"],\n\
          \        )\n\n        # log textual metrics info as well\n        for metric\
          \ in metrics_list[0].keys():\n            if metric != \"confidenceMetrics\"\
          :\n                val_string = json.dumps(metrics_list[0][metric])\n  \
          \              metrics.log_metric(metric, val_string)\n\n    logging.getLogger().setLevel(logging.INFO)\n\
          \n    # extract the model resource name from the input Model Artifact\n\
          \    model_resource_path = model.metadata[\"resourceName\"]\n    logging.info(\"\
          model path: %s\", model_resource_path)\n\n    # Get the trained model resource\n\
          \    model = aiplatform.Model(model_resource_path)\n\n    # Get model evaluation\
          \ metrics from the the trained model\n    eval_name, metrics_list, metrics_str_list\
          \ = get_eval_info(model)\n    logging.info(\"got evaluation name: %s\",\
          \ eval_name)\n    logging.info(\"got metrics list: %s\", metrics_list)\n\
          \    log_metrics(metrics_list, metricsc)\n\n    thresholds_dict = json.loads(thresholds_dict_str)\n\
          \    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n\
          \    if deploy:\n        dep_decision = \"true\"\n    else:\n        dep_decision\
          \ = \"false\"\n    logging.info(\"deployment decision is %s\", dep_decision)\n\
          \n    return (dep_decision,)\n\n"
        image: gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest
pipelineInfo:
  name: classification-model-eval-metrics
root:
  dag:
    outputs:
      artifacts:
        classification-model-eval-metrics-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: classification-model-eval-metrics
        classification-model-eval-metrics-metricsc:
          artifactSelectors:
          - outputArtifactKey: metricsc
            producerSubtask: classification-model-eval-metrics
        metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: classification-model-eval-metrics
        metricsc:
          artifactSelectors:
          - outputArtifactKey: metricsc
            producerSubtask: classification-model-eval-metrics
      parameters:
        dep_decision:
          valueFromParameter:
            outputParameterKey: dep_decision
            producerSubtask: classification-model-eval-metrics
    tasks:
      classification-model-eval-metrics:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-classification-model-eval-metrics
        inputs:
          artifacts:
            model:
              componentInputArtifact: model
          parameters:
            location:
              componentInputParameter: location
            project:
              componentInputParameter: project
            thresholds_dict_str:
              componentInputParameter: thresholds_dict_str
        taskInfo:
          name: classification-model-eval-metrics
  inputDefinitions:
    artifacts:
      model:
        artifactType:
          schemaTitle: system.Artifact
          schemaVersion: 0.0.1
    parameters:
      location:
        parameterType: STRING
      project:
        parameterType: STRING
      thresholds_dict_str:
        parameterType: STRING
  outputDefinitions:
    artifacts:
      classification-model-eval-metrics-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      classification-model-eval-metrics-metricsc:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      metricsc:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
    parameters:
      dep_decision:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
